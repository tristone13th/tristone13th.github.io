---
categories: 操作系统
title: 用户态、内核态、系统调用、syscall、system call
published: true
---

内核态和用户态是怎样实现的？这个问题看似简单，其实不然。

首先从一个简单的回答说起，一般来说，为了应付面试，我们可以说内核态和用户态是基于 CPU 实现的，x86 架构下定义了四个特权级，分别是 Ring 0 到 Ring 3，其中 Ring 0 具有最高的优先级，一般用作内核态；而 Ring 3 的优先级最低，一般用作用户态。对于特权级，我们可以展开说说其实现机制。

### CPU 对特权级的定义及处理

特权级有四级，分别是 0 到 3，越小的特权级越高。特权级还可以细分为

- CPL，是当前 **进程** 的权限级别（Current Privilege Level），是当前正在执行的代码所在的段（也就是 cs 段）的特权级，代表着这个进程的特权级，存在于 cs 寄存器的低两位。CPL 其实指的就是我们说的 ring，也就是说，ring 在硬件上的反映其实就是 cs 寄存器的低两位，参考 [operating system - Is an x86 CPU in kernel mode when the CPL value of the CS register is equal to 0? - Stack Overflow](https://stackoverflow.com/questions/55506822/is-an-x86-cpu-in-kernel-mode-when-the-cpl-value-of-the-cs-register-is-equal-to-0)。
- RPL，说明的是进程对段访问的请求权限（Request Privilege Level），是对于段选择子（保护模式）而言的，每个段选择子有自己的 RPL，它说明的是进程对段访问的请求权限。而且 RPL 对每个段来说不是固定的，两次访问同一段时的 RPL 可以不同。
- DPL，说明的是进程对段访问的权限，规定访问该段的权限级别（Descriptor Privilege Level），每个段的 DPL 固定。

对于 RPL 与 DPL 之间的区别可以访问 [memory segmentation - Difference between DPL and RPL in x86 - Stack Overflow](https://stackoverflow.com/questions/36617718/difference-between-dpl-and-rpl-in-x86)。

### 防止用户态代码更改访问级别是如何实现的？

如果一个用户态代码想要进行一些必要的文件读取操作，那么它必须先切换到内核态才有足够的权限。一般来说，如我我们对于内核态/用户态以及它们之间的切换不够了解，那么我们会自然地认为用户态代码切换到内核态需要以下步骤：

- 内核存在能够从用户态切换到内核态的例程；
- 用户态代码执行系统调用；
- 在调用系统调用的过程中，调用权限切换例程，完成权限的切换。

正常来说，这个步骤是没有问题的。但是，有没有一种可能，用户态进程可以自己实现这个权限切换例程，从而偷取权限，执行一些能够毁坏系统的代码？

答案当然是不可能的，权限等级的设计就是要保证权限切换的方向是单向的，也就是只能够从高优先级切换到低优先级，而不能从低优先级切换到高优先级。

那么如果这是不可能的，我们的系统调用又是如何通过执行例程，在当前特权等级较低的情况下，完成向高特权等级的切换的呢？

所以真实的系统调用流程是这样的：

- 用户态代码以 **中断** 的形式，进行系统调用；
- CPU 在响应中断的同时，自动将特权级别设置为 0，这是一种硬件机制，参考 [operating system - how does an interrupt put CPU into the required privilege level? - Stack Overflow](https://stackoverflow.com/questions/17178716/how-does-an-interrupt-put-cpu-into-the-required-privilege-level)；
- 调用操作系统预先设置的中断响应函数；
- 返回，将特权级别恢复。

可以看到，借助了硬件的实现，用户态进程就不可能自己实现这个权限切换例程，而是只能乖乖地通过中断机制进行系统调用，将代码执行权乖乖地交给内核。

这也是 **系统调用以中断进行调用的原因**。

### `syscall` 指令也是通过中断实现的吗？

From the previous part we know that system call concept is very similar to an interrupt. Furthermore, system calls are implemented as software interrupts. So, when the processor handles a syscall instruction from a user application, this instruction causes an exception which transfers control to an exception handler.

[How the Linux kernel handles a system call · Linux Inside](https://0xax.gitbooks.io/linux-insides/content/SysCall/linux-syscall-2.html)

The syscall instruction causes an exception, which transfers control to an exception handler.

[syscall](https://chortle.ccsu.edu/assemblytutorial/Chapter-22/ass22_2.html)

### Syscall instruction

SYSCALL invokes an OS system-call handler at privilege level 0. It does so by loading RIP from the IA32_LSTAR MSR (after saving the address of the instruction following SYSCALL into RCX). (The WRMSR instruction ensures that the IA32_LSTAR MSR always contain a canonical address.)

From SDM.

So if you want to call a procedure, you should first write the RIP of the procedure to the MSR, then `syscall`.

### Syscall 在 Linux kernel 中的实现

\_\_NR 开头的宏是一个系统调用号的宏定义。

```c
#define __NR_memfd_restricted 451
__SYSCALL(__NR_memfd_restricted, sys_memfd_restricted)

asmlinkage long sys_memfd_restricted(unsigned int flags);

SYSCALL_DEFINE1(memfd_restricted, unsigned int, flags)
{
    //...
}
```

All system calls are marked with the `asmlinkage` tag, so they all look to the stack for arguments.

[c - What is the 'asmlinkage' modifier meant for? - Stack Overflow](https://stackoverflow.com/questions/10459688/what-is-the-asmlinkage-modifier-meant-for)

`SYSCALL_DEFINE()` is a macro that is used to define the implementation of a system call function.

`__SYSCALL()` is a low-level macro that is used to declare the interface to a system call and is used by the kernel to generate the system call table.

The main entry point for your new xyzzy(2) system call will be called sys_xyzzy(), but you add this entry point with the appropriate SYSCALL_DEFINEn() macro rather than explicitly.

`SYSCALL_DEFINEn()`. The ‘n’ indicates the number of arguments to the system call.

[Adding a New System Call — The Linux Kernel documentation](https://www.kernel.org/doc/html/next/process/adding-syscalls.html#generic-system-call-implementation)

Take this as an example: [[PATCH v10 1/9] mm: Introduce memfd_restricted system call to create restricted user memory - Chao Peng](https://lore.kernel.org/all/20221202061347.1070246-2-chao.p.peng@linux.intel.com/)

A call from userspace to `syscall()` function actually won't get the return value you defined. Because you are calling the wrapper function which is:

```c
int syscall(num, ...)
{
    // ...
    int rc = /* arch-specific code to get the result of the system call */ ;
    if (rc < 0) { errno = -rc; return -1; }
    return 0;
}
```

So the actual return value is in `errno`, if error, `syscall()` will always return -1 no matter what the real return value is.

### Syscall vs. int80

A system call number is a unique integer (i.e., whole number), from one to around 256, that is assigned to each system call in a Unix-like operating system.

- `syscall` is the default way of entering kernel mode on `x86-64`. This instruction is not available in 32 bit modes of operation *on Intel processors*.
- `sysenter` is an instruction most frequently used to invoke system calls in 32 bit modes of operation. It is similar to `syscall`, a bit more difficult to use though, but that is the kernel's concern.
- `int 0x80` is a legacy way to invoke a system call and should be avoided.

`int 0x80` does work in some cases in 64-bit code, but is never recommended.

[Chromium OS Docs - Linux System Call Table](https://chromium.googlesource.com/chromiumos/docs/+/master/constants/syscalls.md)

[assembly - What is better "int 0x80" or "syscall" in 32-bit code on Linux? - Stack Overflow](https://stackoverflow.com/questions/12806584/what-is-better-int-0x80-or-syscall-in-32-bit-code-on-linux)

### Can a syscall number be different between x86 and x64?

Yes, they may different.

`fork` in x86 is 2, in x64 is 57.

### System call vs. context switch

[process - System call and context switch - Stack Overflow](https://stackoverflow.com/questions/9238326/system-call-and-context-switch)

In a nutshell: context contains many parts, some system calls just need to switch few parts and other system calls need to switch more parts, which makes it heavier.

### Context switch

上下文切换发生的时机：

- **系统调用**：当用户程序调用系统调用时，系统需要从用户程序的上下文切换到内核态，执行系统调用的操作。
- **中断**：当硬件设备需要与操作系统交互时，系统会触发一个中断，从而进行上下文切换。
- **进程调度**：当操作系统需要调度多个进程执行时，它会进行上下文切换。

**Why system call needs context switch?**

A system call does not necessarily require a context switch in general, but rather a privilege switch. This is because the kernel memory is mapped in each process memory.

[Do system calls always means a context switch? - Computer Science Stack Exchange](https://cs.stackexchange.com/questions/83246/do-system-calls-always-means-a-context-switch)

**System calls are very light compared to process context switching, about 100ns or less.**

[The cost of context switching - SoByte](https://www.sobyte.net/post/2022-06/ctx-switch/)

[Tsuna's blog: How long does it take to make a context switch?](https://blog.tsunanet.net/2010/11/how-long-does-it-take-to-make-context.html)

### 系统调用的 context switch 会改 CR3 吗？/ 内核页表 / 进程页表

答案是：老 CPU 老 kernel，会改；老 CPU 新 kernel，不会改，新 CPU 新 kernel，会改。至于原因请看下面：

进程从用户态进入内核态**不会**引起 CR3 的改变但会引起堆栈的改变。之所以不会引起 CR3 的改变，是因为内核页表和进程页表并不是在独立的两片区域，而是被 **merge 在了同一片区域**，也就是共享同一个 CR3。比如说一个页表所映射的 0-3G 是用户空间页表，但是 3-4G 是内核空间页表，PSE/PTE 有对应的 U/S bit 来表示这个页表项是 supervisor 还是 user 的，如果 User 试图访问内核页表所映射的内存，那么就会 crash。

>每个进程的页面目录就分成了两部分，第一部分为“用户空间”，用来映射其整个进程空间（0x0000 0000－0xBFFF FFFF）即 3G 字节的虚拟地址；第二部分为“系统空间”，用来映射（0xC000 0000－0xFFFF FFFF）1G 字节的虚拟地址。可以看出 Linux 系统中每个进程的页面目录的**第二部分是相同的**，所以从进程的角度来看，每个进程有 4G 字节的虚拟空间， 较低的 3G 字节是自己的用户空间，最高的 1G 字节则为与所有进程以及内核共享的系统空间。

[进入内核态CR3会修改吗？ - 知乎](https://zhuanlan.zhihu.com/p/46032839)

但是在 Meltdown 这个漏洞出现之后，就变了：[内核页表隔离 - 维基百科，自由的百科全书](https://zh.wikipedia.org/wiki/%E5%86%85%E6%A0%B8%E9%A1%B5%E8%A1%A8%E9%9A%94%E7%A6%BB)，基于此我们有了内核页表隔离技术（KPTI）：

如果没有 KPTI，每当执行用户空间代码（应用程序）时，Linux 会在其分页表中保留整个内核内存的映射，并保护其访问。**这样做的优点是当应用程序向内核发送系统调用或收到中断时，内核页表始终存在，可以避免绝大多数上下文切换相关的开销（TLB 刷新、页表交换也就是 CR3 交换等）**。

KPTI 通过完全分离用户空间与内核空间页表来解决页表泄露。支持进程上下文标识符（PCID）特性的 x86 处理器可以用它来**避免 TLB 刷新**，但即便如此，它依然有很高的性能成本：

- 据 KAISER 原作者称，其开销为 0.28%；
- 一名 Linux 开发者称大多数工作负载下测得约为 5%。
- 但即便有 PCID 优化，在某些情况下开销高达 30%。

KPTI 在 2018 年早期被合并到 Linux 内核 4.15 版，使用内核启动选项 `pti=off` 可以部分禁用内核页表隔离。依规定也**可对已修复漏洞的新款处理器禁用 KPTI**。

有用的参考：context switch^。

内核页表和用户页表不同，它一般不做 swap，也就没有 page fault，而且它一般不会把连续的虚拟地址空间映射成不连续的物理空间，一般只是做一个 offset。这样有一个好处，内核代码比如 driver 如果要拿到一个 VA 所对应的 PA，不需要去遍历页表，只需要通过 offset 来加就可以了，性能上会更好。

所有进程**内核态空间到物理地址空间的映射**是相同的。

即使内核代码执行的指令操作数也是虚拟地址而不是物理地址。

### 内核线程

内核线程（是不是叫内核进程比较好？）是直接由内核本身启动的进程。内核线程实际上是将**内核函数**委托给**独立的进程**，它与内核中的其他进程”并行”执行。

和其他用户空间的线程的区别应该在于 code 不一样吧：用户空间线程的代码肯定有很多 `SYSENTER` 和 `SYSRET` 指令，也就导致一会跑在用户态一会跑在内核态；但是内核线程没有用这些指令，一直跑在内核态。
