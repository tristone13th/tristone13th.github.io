---
categories: 深度学习
title: 神经网络
---

# 表示方法

|       符号       |                 含义                 |
| :--------------: | :----------------------------------: |
|     $n_{l}$      |               网络层数               |
|     $L_{l}$      |               第$l$层                |
|  $W_{ij}^{(l)}$  |  $l$层$j$节点到$l+1$层$i$节点的权值  |
|  $b_{i}^{(l)}$   |     第$l+1$层第$i$个节点的偏置项     |
|   $a_{i}^{l}$    | 第$l$层第$i$个节点的输出值（激活值） |
|   $z_{i}^{l}$    |    第$l$层第$i$个节点的输入加权和    |
|   $h_{W,b}(x)$   |          对于所有输入的输出          |
|     $s_{l}$      |           第$l$层的节点数            |
| $\delta_{i}^{l}$ |       第$l$层第$i$个节点的残差       |

# 网络结构类型

## 前馈神经网络

应当注意，`BP`神经网络也是前馈神经网络的一种，因为其并没有让输出值作为输入，而只是将参数学习策略加入了反馈而已。

## 自编码神经网络

自编码神经网络是一种无监督学习算法，它将标签`label`设置为输入，尝试逼近一个恒等函数，并迫使神经网络去学习一个自我表示的模型。

这样的一个自编码器看起来毫无意义，但是当我们为自编码神经网络加入某些限制，比如限定隐藏神经元的数量，我们就可以从输入数据中发现一些有趣的结构。

事实上，这一简单的自编码神经网络通常可以学习出一个跟主元分析`PCA`结果非常相似的输入数据的低维表示。

## 循环神经网络

`RNNs`的目的使用来处理序列数据。在传统的神经网络模型中，是从输入层到隐含层再到输出层，层与层之间是全连接的，每层之间的节点是无连接的。但是这种普通的神经网络对于很多问题却无能无力。

例如，你要预测句子的下一个单词是什么，一般需要用到前面的单词，因为一个句子中前后单词并不是独立的。`RNNs`之所以称为循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。

理论上，`RNNs`能够对任何长度的序列数据进行处理。但是在实践中，为了降低复杂性往往假设当前的状态只与前面的几个状态相关。

# 算法策略

## 对称失效

在对参数进行初始化时，应当避免将其初始化为相同的值，因为这样会使得第$l​$层的每一个神经元都获得了相同的输入值，故应当对参数进行随机初始化，即将参数随机化为一个接近于零的数字。

## 反向传播算法

代价函数大多数使用平方距离来进行计算。单个样例的代价函数可以表示为


$$
J(W, b ; x, y)=\frac{1}{2}\left\|h_{W, b}(x)-y\right\|^{2}
$$


总样例的代价函数即为$J(W, b)$，在此不再进行展开。

我们的权值更新基于梯度下降来进行更新，并由此规定学习率为$\alpha$，在进行梯度下降学习算法时，需要求导数以计算梯度。注意对于两个不同类别的参数$w, b$，**有不同的更新算法**。在对参数$w$进行更新时，需要加上正则化项以防止函数的过拟合。

### 参数$W$的更新策略

我们应当根据参数误差来对我们的目标函数进行优化（梯度下降）。首先考虑当我们要对参数$W_{i j}^{(l)}$进行更新时，我们应当**使用全局误差函数对其进行求导**，即根据函数：


$$
\frac{\delta}{\delta W_{i j}^{(l)}} J(W, b)
$$


来进行误差的更新，然而这么一个误差函数如何对参数$W_{i j}^{(l)}$进行求导呢？我们可以考虑到


$$
\frac{\delta}{\delta W_{i j}^{(l)}} J(W, b)=\frac{\delta}{\delta z_{i}^{l+1}} J(W, b) \times \frac{\delta z_{i}^{l+1}}{\delta W_{i j}^{(l)}}
$$


这是求导的传递法则，进而我们不难发现


$$
\frac{\delta z_{i}^{l+1}}{\delta W_{i j}^{(l)}}=a_{j}^{(l)}, \frac{\delta}{\delta z_{i}^{l+1}} J(W, b)=\delta_{i}^{(l+1)}
$$


带入以上公式得到


$$
\frac{\delta}{\delta W_{i j}^{(l)}} J(W, b)=a_{j}^{(l)} \delta_{i}^{(l+1)}
$$


根据该公式我们就可以对参数$W_{i j}^{(l)}$进行更新了。

> 注：$W_{i j}^{l}$代表的是第$l$层第$j$个单元到第$l + 1$层第$i$个单元的权值，切勿弄反了。

### 参数$b$的更新策略

与计算参数$W$相同，我们也应当计算


$$
\frac{\delta}{\delta b_{i}^{(l)}} J(W, b)
$$
来对参数进行更新。

首先，由于第$l$层第$i$个神经元输出与输入相等，我们考虑计算


$$
\frac{\delta}{\delta b_{i}^{(l)}} J(W, b)=\frac{\delta}{\delta z_{i}^{(l+1)}} J(W, b) \times \frac{\delta z_{i}^{(l+1)}}{\delta b_{i}^{(l)}}
$$


因为计算式


$$
\frac{\delta z_{i}^{(l+1)}}{\delta b_{i}^{(l)}}=1
$$


故我们得到


$$
\frac{\delta}{\delta b_{i}^{(l)}} J(W, b)=\frac{\delta}{\delta z_{i}^{(l+1)}} J(W, b)=\delta_{i}^{l+1}
$$


据此进行更新即可。