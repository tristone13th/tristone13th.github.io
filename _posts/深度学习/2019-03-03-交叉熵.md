---
categories: 深度学习
title: 交叉熵
---

# 概述 

在信息论中，交叉熵是表示两个概率分布$p$、$q$，其中$p$表示真实分布，$q$表示非真实分布，在相同的一组事件中，其中，用非真实分布$q$来表示某个事件发生所需要的平均比特数。也就是两个分布的相似度。

# 编码长度

信息论中，熵代表着根据信息的**概率分布**对信息**编码**所需要的**最短平均编码长度**。

举个简单的例子来理解一下这件事情：假设有个考试作弊团伙，需要连续不断地向外传递四选一单选题的答案。直接传递`ABCD`的`ASCII`码的话，每个答案需要`8bits`的二进制编码，从传输的角度，这显然有些浪费。信息论最初要解决的，就是数据压缩和传输的问题，所以这个作弊团伙希望能用更少`bit`的编码来传输答案。很简单，答案只有`4`种可能性，所以二进制编码需要的长度就是$\log_{2}4=2$。

在上面的例子中，隐含了一种假设，即四个答案出现概率是相等的，均为$p=1/4$，所以编码需要长度的计算可以理解为如下的形式：


$$
H(X)=E_{x\sim p(x)}(log_{2}\frac{1}{p(x)})
$$


由此可见，对于事件$x_{i}$，该事件所需的最短编码长度为$log_{2} (1/p(x_{i}))$，那么对于所有的事件，我们能够得出总最短平均编码长度为


$$
H(X)=\sum_{i}p(x_{i})(log_{2}\frac{1}{p(x_{i})})
$$



# 定义 

假设现在有一个样本集中两个概率分布$p$、$q$，其中$p$为真实分布，$q$为非真实分布。假如，按照真实分布$p$来衡量识别一个样本所需要的编码长度的期望为：


$$
H(p)=\sum_{i}p(i)log_{2}\frac{1}{p(i)}
$$


但是，若是采用了错误的分布$q$来表示来自真实分布$p$的编码长度，即使用了分布$q$进行了编码，那么平均编码程度则应该是：


$$
H(p,q)=\sum_{i}p(i)log_{2}\frac{1}{q(i)}
$$


此时将$H(p,q)$称为交叉熵。

# 应用

当非真实分布$q$接近于真实分布$p$时，交叉熵即为最短编码长度，由于其“最短性”，其他任意一个分布对于真实分布的交叉熵都不会小于该真实分布的最短编码长度。这也是交叉熵可在神经网络(机器学习)中作为损失函数的原因。
