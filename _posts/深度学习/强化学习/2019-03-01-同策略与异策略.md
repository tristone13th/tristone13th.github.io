---
categories: 强化学习
title: 同策略与异策略
---

# 表示范围

**同策略（On Policy）**与**异策略（Off Policy）**只用于形容通过**采样**进行学习的算法，如蒙特卡洛算法（通过大量的对于**完整轨迹**的采样对值函数进行正确预估）、时序差分学习算法（$SARSA$和$Q$学习算法都是采样一步，更新一步，不需要提前得知一次采样的完整轨迹）。由于在**动态规划算法**算法中，模型已知不需要进行采样，策略的预估不需要进行迭代更新，所以同策略与异策略**不能**用于形容动态规划算法。

同策略算法也称为**在线算法**，异策略算法也称为**离线算法**。

# 同策略

## 定义

如果采样策略是$\pi^{\epsilon}(s)$，通过采样得到的数据不断改进的策略也是$\pi^{\epsilon}(s)$，而不是目标策略$\pi(s)$，这种采样与改进策略相同的强化学习方法叫做**同策略**（On Policy）方法。

## 所含算法

时序差分学习中的$SARSA$算法即为一种同策略的学习算法。

# 异策略

如果采样策略是$\pi^{\epsilon}(s)$，而优化目标策略是策略$\pi$，可以通过重要性采样，引入重要性权重来实现对于目标策略$\pi$的优化。这种采样与改进分别使用不同策略的强化学习方法叫做异策略（Off Policy）方法。

## 所含算法

时序差分学习中的$Q$学习算法即为一种异策略的学习算法。

$$

$$


