---
categories: 强化学习
title: 基于策略函数的学习方法
---
# 简介

强化学习的目标是学习到一个**策略$\pi_{\theta}​$**来最大化期望回报。一种直接的方法是在**策略空间直接搜索**来得到最佳策略，称为策略搜索（Policy Search）。

策略搜索本质是一个**优化问题**，可以分为**基于梯度的优化和无梯度优化**。策略搜索和基于值函数的方法相比，策略搜索可以**不需要值函数，直接优化策略**。参数化的策略能够处理连续状态和动作，可以直接学出随机性策略。

# 策略梯度（Policy Gradient）

策略梯度（Policy Gradient）是一种基于**梯度**的强化学习方法。

假设$\pi_{\theta}$是一个关于$\theta$的连续可微函数，我们可以用梯度上升的方法来优化参数$\theta$使得目标函数$\mathcal{J}(\theta)$最大。

目标函数定义如下，表示执行某策略所能得到的平均回报：


$$
\mathcal{J}(\theta)=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}[G(\tau)]=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t=0}^{T-1} \gamma^{t} r_{t+1}\right]
$$


由此，我们首先求得目标函数对于策略参数$\theta$的导数：


$$
\begin{aligned} \frac{\partial \mathcal{J}(\theta)}{\partial \theta} &=\frac{\partial}{\partial \theta} \int p_{\theta}(\tau) G(\tau) d \tau \\ &=\int\left(\frac{\partial}{\partial \theta} p_{\theta}(\tau)\right) G(\tau) d \tau \\ &=\int p_{\theta}(\tau)\left(\frac{1}{p_{\theta}(\tau)} \frac{\partial}{\partial \theta} p_{\theta}(\tau)\right) G(\tau) d \tau \\ &=\int p_{\theta}(\tau)\left(\frac{\partial}{\partial \theta} \log p_{\theta}(\tau)\right) G(\tau) d \tau \\ &=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[\frac{\partial}{\partial \theta} \log p_{\theta}(\tau) G(\tau)\right] \end{aligned}
$$


其中，$\frac{\partial}{\partial \theta} \log p_{\theta}(\tau)$可以进一步分解为


$$
\begin{aligned} \frac{\partial}{\partial \theta} \log p_{\theta}(\tau) &=\frac{\partial}{\partial \theta} \log \left(p\left(s_{0}\right) \prod_{t=0}^{T-1} \pi_{\theta}\left(a_{t} | s_{t}\right) p\left(s_{t+1} | s_{t}, a_{t}\right)\right) \\ &=\frac{\partial}{\partial \theta}\left(\log p\left(s_{0}\right)+\sum_{t=0}^{T-1} \log \pi_{\theta}\left(a_{t} | s_{t}\right)+\log p\left(s_{t+1} | s_{t}, a_{t}\right)\right) \\ &=\sum_{t=0}^{T-1} \frac{\partial}{\partial \theta} \log \pi_{\theta}\left(a_{t} | s_{t}\right) \end{aligned}
$$


其中，第一步将每一个轨迹分成了若干了部分，即一个轨迹出现的概率等于初始状态出现的概率$p(s_{0})\times$通过策略$\pi_{\theta}$下一步选择动作$a_{t}$的概率$\times$选择该动作后状态转移至$s_{t}$的概率。

第二步根据对数函数的性质，括号内的所有乘号变为加号。

第三步将括号内的前后两项剔除（对于参数$\theta$来说是常数），得到最后的和公式。**可以看出，$\frac{\partial}{\partial \theta} \log p_{\theta}(\tau)$是和状态转移概率无关，只和策略函数相关。**

因此，策略梯度$\frac{\partial \mathcal{J}(\theta)}{\partial \theta}$可以再写为：


$$
\begin{aligned} \frac{\partial \mathcal{J}(\theta)}{\partial \theta} &=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[\left(\sum_{t=0}^{T-1} \frac{\partial}{\partial \theta} \log \pi_{\theta}\left(a_{t} | s_{t}\right)\right) G(\tau)\right] G(\tau) ] \\ &=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[\left(\sum_{t=0}^{T-1} \frac{\partial}{\partial \theta} \log \pi_{\theta}\left(a_{t} | s_{t}\right)\right)\left(G\left(\tau_{1 : t-1}\right)+\gamma^{t} G\left(\tau_{t : T}\right)\right)\right] \\ &=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t=0}^{T-1}\left(\frac{\partial}{\partial \theta} \log \pi_{\theta}\left(a_{t} | s_{t}\right) \gamma^{t} G\left(\tau_{t : T}\right)\right)\right] \end{aligned}
$$


