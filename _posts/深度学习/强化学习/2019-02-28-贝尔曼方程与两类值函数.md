---
categories: 强化学习
title: 贝尔曼方程与两类值函数
---


# 贝尔曼方程与两类值函数

为了评估一个策略$\pi$的期望回报，我们定义两个值函数：**状态值函数**和**状态-动作值函数**。

## 状态值函数

### 折扣率的引入

#### 有终止状态的情况

总回报的引入方式如下：


$$
G(\tau)=\sum_{t=0}^{T-1} r_{t+1}=\sum_{t=0}^{T-1} r\left(s_{t}, a_{t}, s_{t+1}\right)
$$


假设环境中有**一个或多个**终止状态，当到达终止状态时，一个智能体和环境的交互就结束了。这一轮的交互过程称为一个**回合（episode）**或**试验（trial）**。

#### 没有终止状态的情况

如果环境中没有终止状态(比如终身学习的机器人)，即$T=\infty$，称为持续性强化学习任务，其总回报也可能是无穷大。

为了解决这个问题，我们可以引入一个折扣率来降低远期回报的比重。**折扣回报**定义为


$$
G(\tau)=\sum_{t=0}^{T-1} \gamma^{t} r_{t+1}
$$


其中，$\gamma$代表折扣率，其取值范围在零到一之间。

### 状态值函数的计算

状态值函数表示在某一状态$s​$下，执行一个策略到最终状态所能够得到的**总回报**，数学公式使用$V^{\pi}(s)​$来进行表示。

一个策略$\pi$的总期望回报，可以通过以下公式进行计算：


$$
\begin{aligned} \mathbb{E}_{\tau \sim p(\tau)}[G(\tau)] &=\mathbb{E}_{s \sim p\left(s_{0}\right)}\left[\mathbb{E}_{\tau \sim p(\tau)} \sum_{t=0}^{T-1} \gamma^{t} r_{t+1} | \tau_{s_{0}}=s\right] ] \\ &=\mathbb{E}_{s \sim p\left(s_{0}\right)}\left[V^{\pi}(s)\right], \end{aligned}
$$


其中，状态值函数$V^{\pi}(s)$可以通过如下来计算：


$$
V^{\pi}(s)=\mathbb{E}_{\tau \sim p(\tau)}\left[\sum_{t=0}^{T-1} \gamma^{t} r_{t+1} | \tau_{s_{0}}=s\right]
$$


这个公式的意思是：从状态$s$出发所能得到的总回报等于以状态$s$为初始状态的所有可能路径的回报的期望。根据马尔科夫性，$V^{\pi}(s)$可展开得到：


$$
V^{\pi}(s)=\mathbb{E}_{a \sim \pi(a | s)} \mathbb{E}_{s^{\prime} \sim p\left(s^{\prime} | s, a\right)}\left[r\left(s, a, s^{\prime}\right)+\gamma V^{\pi}\left(s^{\prime}\right)\right]
$$


该公式称为**贝尔曼方程**。表示当前状态的**值函数**可以通过下个状态的**值函数**来计算。

## 状态动作值函数

初始状态为$s$并进行动作$a$，然后执行策略$\pi$得到的**期望总回报**，称为状态动作值函数，也称为$Q$函数。


$$
Q^{\pi}(s, a)=\mathbb{E}_{s^{\prime} \sim p\left(s^{\prime} | s, a\right)}\left[r\left(s, a, s^{\prime}\right)+\gamma V^{\pi}\left(s^{\prime}\right)\right]
$$


该公式表示在状态$s$下，执行动作$a$得到的期望回报$Q^{\pi}(s, a)$为对于执行动作$a$后的下一可能状态$s^{\prime}$的值函数$V^{\pi}\left(s^{\prime}\right)$的折扣期望加上该次获得的奖励$r(s,a,s^{\prime})$。

又由于**状态值函数$V^{\pi}\left(s\right)$**是$Q$函数$Q^{\pi}(s, a)$关于动作$a$的期望：


$$
V^{\pi}(s)=\mathbb{E}_{a \sim \pi(a | s)}\left[Q^{\pi}(s, a)\right]
$$


结合上述公式，可以将$Q$函数写为：


$$
Q^{\pi}(s, a)=\mathbb{E}_{s^{\prime} \sim p\left(s^{\prime} | s, a\right)}\left[r\left(s, a, s^{\prime}\right)+\gamma \mathbb{E}_{a^{\prime} \sim \pi\left(a^{\prime} | s^{\prime}\right)}\left[Q^{\pi}\left(s^{\prime}, a^{\prime}\right)\right]\right]
$$


这是关于$Q$函数的贝尔曼方程。

